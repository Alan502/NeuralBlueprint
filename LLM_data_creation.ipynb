{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM data creation\n",
        "\n",
        "This notebook adapts the llm-data-creation github repository to show how the code works. It focuses on the data_creation_tree.py code, which per the published paper is the most effective strategy for questions that are most similar as the starting data. This notebook adapts the code to make it work with the Gemini API."
      ],
      "metadata": {
        "id": "5IuxYYx4Gvij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency setup\n",
        "\n",
        "To run the code below, make sure that you have set the `GOOGLE_API_KEY`as a Google Colab secret."
      ],
      "metadata": {
        "id": "YMN3rp72Iiwb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8D9JXJqfvDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420e7d9f-5054-403e-f4e8-dd2acbb9f378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-generativeai==0.8.3 in /usr/local/lib/python3.10/dist-packages (0.8.3)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (0.6.10)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (2.19.2)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (2.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai==0.8.3) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai==0.8.3) (1.65.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai==0.8.3) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai==0.8.3) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai==0.8.3) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai==0.8.3) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai==0.8.3) (2.23.4)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai==0.8.3) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai==0.8.3) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai==0.8.3) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai==0.8.3) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U google-generativeai==0.8.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QN-3mEdyf5TE"
      },
      "outputs": [],
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata\n",
        "\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query setup\n",
        "\n",
        "This function sets up the LLM querying, including the model and other configuration attributes."
      ],
      "metadata": {
        "id": "MramsvWWJda2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKWhw63nhu2E"
      },
      "outputs": [],
      "source": [
        "def api_query(description: str, text: str, model: str = \"gemini-1.5-flash\") -> str:\n",
        "    model = genai.GenerativeModel(model, system_instruction=description)\n",
        "    config = genai.types.GenerationConfig(temperature=1, top_p=0)\n",
        "    response = model.generate_content(text, generation_config=config, request_options={\"timeout\": 1000})\n",
        "    return_text = response.text\n",
        "    return return_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt setup\n",
        "\n",
        "### Prompt type\n",
        "\n",
        "The paper explains two main ways of setting up a prompt, depending on the type of data that you wish to generate. The first type is \"fix\" which sets up a question with a fixed set of answers, the LLM is asked to always pick from the fixed set of answers. The second type is \"variant\", this allows the LLM to pick its own list of answers, and pick the one that is correct.\n",
        "\n",
        "### Root example\n",
        "\n",
        "To get the data generation started, a root node example must be set. For this, we will need a question, options and an answer.\n",
        "\n",
        "### Context\n",
        "\n",
        "Optionally, a context can be set for the question. The paper explains that setting a context will make the LLM generate a context for answering the question, and it will perform better with reasoning tasks."
      ],
      "metadata": {
        "id": "5hRaU3LYLXCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_type = \"variant\" # one of \"variant\" or \"fix\"\n",
        "question = \"What liquid can be kept in a large container?\"\n",
        "options = [\"juice\", \"door\", \"shed\", \"supermarket\", \"cabinet\"]\n",
        "answer = \"refrigerator\" # answer must be one of the\n",
        "context=None"
      ],
      "metadata": {
        "id": "GGHg-WI2O1yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tree size\n",
        "\n",
        "The following parameters decide the amount of data to generate.\n",
        "\n",
        "### Number examples\n",
        "\n",
        "This parameter sets the number of examples to ask for from the LLM during each iteration.\n",
        "\n",
        "### Length of train\n",
        "\n",
        "Length of train sets the total number of examples to generate. This should be more than num_examples, and ideally a multiplier of num_examples."
      ],
      "metadata": {
        "id": "3exBWGtVTE7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 3\n",
        "len_train = 9"
      ],
      "metadata": {
        "id": "lpgbXfS3TuRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generation algorithm\n",
        "\n",
        "The following code has the generation algorithm! It will use a tree-based algorithm to generate more and more examples based on the initial example."
      ],
      "metadata": {
        "id": "ub4zIYWtVHT7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "o1Rph4UZJdg-",
        "outputId": "5f9f784b-23a2-440a-9f2f-1c2ec2461c11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "- You are creating 3 more examples that follow the format of the example provided, but with a different content.\n",
            "- The created examples **must** all have different answers.\n",
            "- The output **must** be in unnumbered JSON format.\n",
            "\n",
            "size of previous tree: 1\n",
            "{\"question\": \"What is the opposite of hot?\", \"context\": null, \"options\": [\"cold\", \"warm\", \"cool\", \"freezing\", \"ice\"], \"label\": \"cold\"}\n",
            "\n",
            "{\"question\": \"What is the name of the animal that barks?\", \"context\": null, \"options\": [\"cat\", \"dog\", \"bird\", \"fish\", \"snake\"], \"label\": \"dog\"}\n",
            "\n",
            "{\"question\": \"What is the name of the planet we live on?\", \"context\": null, \"options\": [\"Mars\", \"Venus\", \"Jupiter\", \"Saturn\", \"Earth\"], \"label\": \"Earth\"}\n",
            "\n",
            "depth: 0, count: 3\n",
            "size of previous tree: 3\n",
            "{\"question\": \"What is the opposite of up?\", \"context\": null, \"options\": [\"down\", \"left\", \"right\", \"forward\", \"backward\"], \"label\": \"down\"}\n",
            "\n",
            "{\"question\": \"What is the opposite of happy?\", \"context\": null, \"options\": [\"sad\", \"angry\", \"excited\", \"tired\", \"bored\"], \"label\": \"sad\"}\n",
            "\n",
            "{\"question\": \"What is the opposite of wet?\", \"context\": null, \"options\": [\"dry\", \"damp\", \"moist\", \"sticky\", \"soapy\"], \"label\": \"dry\"}\n",
            "\n",
            "depth: 1, count: 6\n",
            "{\"question\": \"What is the largest planet in our solar system?\", \"context\": null, \"options\": [\"Mars\", \"Venus\", \"Jupiter\", \"Saturn\", \"Mercury\"], \"label\": \"Jupiter\"}\n",
            "\n",
            "{\"question\": \"What is the smallest planet in our solar system?\", \"context\": null, \"options\": [\"Mars\", \"Venus\", \"Jupiter\", \"Saturn\", \"Mercury\"], \"label\": \"Mercury\"}\n",
            "\n",
            "{\"question\": \"What is the hottest planet in our solar system?\", \"context\": null, \"options\": [\"Mars\", \"Venus\", \"Jupiter\", \"Saturn\", \"Mercury\"], \"label\": \"Venus\"}\n",
            "\n",
            "depth: 1, count: 9\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "from textwrap import dedent\n",
        "from typing import List, Optional\n",
        "\n",
        "\n",
        "def system_instruction(num_examples: int = 3, prompt_type: str = \"fix\") -> str:\n",
        "    if prompt_type == \"fix\":\n",
        "        instruction = dedent(\n",
        "            f\"\"\"\n",
        "            - You are creating {num_examples} more examples that follow the format of the example provided, but with a different content.\n",
        "            - The created examples **must** all have different answers.\n",
        "            - The created examples **must** have the same options as the provided example.\n",
        "            - The output **must** be in unnumbered JSON format.\n",
        "            \"\"\"\n",
        "        )\n",
        "    else:\n",
        "        instruction = dedent(\n",
        "            f\"\"\"\n",
        "            - You are creating {num_examples} more examples that follow the format of the example provided, but with a different content.\n",
        "            - The created examples **must** all have different answers.\n",
        "            - The output **must** be in unnumbered JSON format.\n",
        "            \"\"\"\n",
        "        )\n",
        "    return instruction\n",
        "\n",
        "\n",
        "def example_instruction(\n",
        "    label_space: List[str],\n",
        "    question: str,\n",
        "    answer: str,\n",
        "    context: Optional[str],\n",
        "    prompt_type: str = \"fix\",\n",
        ") -> str:\n",
        "    if prompt_type == \"fix\":\n",
        "        if context:\n",
        "            prompt = dedent(\n",
        "                f\"\"\"\n",
        "                \"Options\": {json.dumps(label_space)},\n",
        "                \"Answer\": \"{answer}\",\n",
        "                \"Question\": \"{question}\",\n",
        "                \"Context\": \"{context}\"\n",
        "                \"\"\"\n",
        "            )\n",
        "        else:\n",
        "            prompt = dedent(\n",
        "                f\"\"\"\n",
        "                \"Options\": {json.dumps(label_space)},\n",
        "                \"Answer\": \"{answer}\",\n",
        "                \"Question\": \"{question}\"\n",
        "                \"\"\"\n",
        "            )\n",
        "    elif prompt_type == \"variant\":\n",
        "        if context:\n",
        "            prompt = dedent(\n",
        "                f\"\"\"\n",
        "                \"Question\": \"{question}\",\n",
        "                \"Context\": \"{context}\",\n",
        "                \"Options\": {json.dumps(label_space)},\n",
        "                \"Answer\": \"{answer}\"\n",
        "                \"\"\"\n",
        "            )\n",
        "        else:\n",
        "            prompt = dedent(\n",
        "                f\"\"\"\n",
        "                \"Question\": \"{question}\",\n",
        "                \"Options\": {json.dumps(label_space)},\n",
        "                \"Answer\": \"{answer}\"\n",
        "                \"\"\"\n",
        "            )\n",
        "\n",
        "    return \"{\" + prompt + \"}\"\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    system_inst = system_instruction(num_examples=num_examples, prompt_type=prompt_type)\n",
        "\n",
        "    example_inst = example_instruction(\n",
        "        label_space=options,\n",
        "        question=question,\n",
        "        answer=answer,\n",
        "        context=context,\n",
        "        prompt_type=prompt_type,\n",
        "    )\n",
        "\n",
        "    print(system_inst)\n",
        "\n",
        "    history = set()\n",
        "    count = 0\n",
        "    depth = 0\n",
        "\n",
        "    prev_tree = [example_inst]\n",
        "    next_tree = []\n",
        "\n",
        "    while count < len_train:\n",
        "        print(f\"size of previous tree: {len(prev_tree)}\")\n",
        "        random.shuffle(prev_tree)\n",
        "        for tree_example_inst in prev_tree:\n",
        "            text_result = api_query(description=system_inst, text=tree_example_inst)\n",
        "            output = text_result.strip().split(\"\\n},\\n{\")\n",
        "            temp_dataset = []\n",
        "            try:\n",
        "                assert len(output) == num_examples\n",
        "                for x in output:\n",
        "                    x = x.strip()\n",
        "                    if x[0] != \"{\":\n",
        "                        x = \"{\" + x\n",
        "                    if x[-1] != \"}\":\n",
        "                        x = x + \"}\"\n",
        "                    json_instance = json.loads(x)\n",
        "\n",
        "                    if len(json_instance[\"Options\"]) != len(options):\n",
        "                        continue\n",
        "                    else:\n",
        "                        if prompt_type == \"fix\" and json_instance[\"Options\"] != options:\n",
        "                            continue\n",
        "\n",
        "                    if json_instance[\"Question\"] in history:\n",
        "                        continue\n",
        "\n",
        "                    if json_instance[\"Answer\"] not in json_instance[\"Options\"]:\n",
        "                        continue\n",
        "\n",
        "                    history.add(json_instance[\"Question\"])\n",
        "\n",
        "                    data_instance = {\n",
        "                        \"question\": json_instance[\"Question\"],\n",
        "                        \"context\": json_instance[\"Context\"] if \"Context\" in json_instance else None,\n",
        "                        \"options\": json_instance[\"Options\"],\n",
        "                        \"label\": json_instance[\"Answer\"],\n",
        "                    }\n",
        "\n",
        "                    temp_dataset.append(data_instance)\n",
        "\n",
        "                for x in temp_dataset:\n",
        "                    next_tree.append(\n",
        "                        example_instruction(\n",
        "                            label_space=x[\"options\"],\n",
        "                            question=x[\"question\"],\n",
        "                            answer=x[\"label\"],\n",
        "                            context=x[\"context\"],\n",
        "                            prompt_type=prompt_type,\n",
        "                        )\n",
        "                    )\n",
        "                    print(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
        "                    count += 1\n",
        "                    if count == len_train:\n",
        "                        break\n",
        "\n",
        "                print(f\"depth: {depth}, count: {count}\")\n",
        "                if count == len_train:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:  # pylint: disable=broad-exception-caught\n",
        "                print(e)\n",
        "\n",
        "        if count == len_train:\n",
        "            break\n",
        "\n",
        "        if len(next_tree) == 0:\n",
        "            continue\n",
        "\n",
        "        prev_tree = next_tree\n",
        "        next_tree = []\n",
        "        depth += 1\n",
        "\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}